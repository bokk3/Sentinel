% Literature Review

The ethical governance of autonomous systems in warfare is a field characterized by a collision of disciplines: science fiction philosophy, classical military ethics, international law, and modern computer science. This review synthesizes these distinct threads to demonstrate that while significant work has been done on ``killer robots'' (offensive LAWS), there remains a critical theoretical vacuum regarding purely defensive, high-speed autonomous systems.

\subsection{The Asimovian Legacy and its Military Inadequacies}
Isaac Asimov's Three Laws of Robotics have served as the default starting point for machine ethics for nearly a century. However, as Susan Leigh Anderson argues, Asimov's laws were never intended as a functional ethical code, but rather as a literary device designed to generate conflict \cite{anderson2008}. Anderson notes that the laws effectively reduce machines to the status of ``ethical slaves,'' a stance that may be philosophically tenable for a household servant but becomes problematic in the chaos of the battlefield.

The primary failure of the Asimovian framework in a military context is the First Law's absolute prohibition on harm (``A robot may not injure a human being''). In a defensive engagement, minimizing overall harm often requires the sanctioned use of force—for example, destroying an incoming missile even if the debris risks injuring a bystander, provided that the alternative (the missile impact) would kill significantly more people. A strict First Law robot would be paralyzed by this ``Trolley Problem,'' unable to act. Furthermore, Asimov’s laws assume a clear distinction between ``human'' and ``non-human,'' a distinction that becomes blurred in modern warfare where combatants are often remote or obscured.

\subsection{Just War Theory: From \textit{Bellum} to \textit{Vim}}
Classical Just War Theory, rooted in the works of Augustine and Aquinas and modernized by Michael Walzer, provides the moral bedrock for Western military practice. Walzer’s distinction between \textit{Jus ad Bellum} (justice of going to war) and \textit{Jus in Bello} (justice in conduct) is crucial \cite{walzer1977}. Defensive AI systems sit firmly within the most accepted precept of \textit{Jus ad Bellum}: the right of self-defense. Unlike offensive systems, which must justify aggression, a defensive system is moral by its very existence, provided it remains defensive.

However, the nature of modern defensive force challenges classical definitions. Daniel Brunstetter introduces the concept of \textit{Jus ad Vim} (force short of war), arguing that modern technologies like drones and precision strikes operate in a ``grey zone'' where the scale of violence does not rise to full-scale war \cite{brunstetter2013}. High-speed intercepts fall into this category—they are acts of violence, but their intent is negation rather than destruction. The ethical framework for AI must therefore navigate this grey zone, ensuring that acts of ``force short of war'' do not inadvertently escalate into full-scale conflict.

\subsection{International Humanitarian Law (IHL) and the Definition of Attack}
The legal constraints on autonomous systems are defined primarily by the Additional Protocols to the Geneva Conventions and customary international law. A critical but often overlooked distinction is found in Article 49 of Additional Protocol I, which defines an ``attack'' as an act of violence against the adversary, whether in offense or defense \cite{icrc}.

\subsubsection{The Distinction Imperative}
The International Court of Justice (ICJ), in its 1996 \textit{Advisory Opinion on the Legality of the Threat or Use of Nuclear Weapons}, affirmed that the principle of distinction—separating combatants from non-combatants—is one of the ``intransgressible principles of international customary law'' \cite{icj1996}. This ruling establishes that no military necessity, not even the survival of the state, can justify a weapon system that is inherently indiscriminate. For defensive AI, this sets a high bar: a system cannot simply fire at high-speed objects; it must possess the discriminatory capability to distinguish an incoming warhead from a civilian airliner with near-perfect certainty. The breakdown of this distinction would render the system's deployment unlawful \textit{ab initio}.

\subsubsection{Defensive Necessity and Proportionality}
The right to deploy such systems is grounded in the inherent right of self-defense, codified in Article 51 of the UN Charter and clarified in the case of \textit{Nicaragua v. United States} \cite{icj1986}. The ICJ ruled that self-defense is justifiable only when it is both ``necessary'' and ``proportional'' to the armed attack.
For a ``Sentinel'' system, this implies that the automated response must be strictly limited to halting the aggression. If an AI system were to ``redirect'' an incoming missile back to its launcher, this would exceed the bounds of strict self-defense and constitute a reprisal or a new attack, potentially violating the proportionality constraint if it risks civilian harm in the aggressor's territory. Thus, the legal architecture demands a system that is technically constrained to ``Neutralization''—rendering the threat inert—rather than ``Counter-Attack.''

The International Committee of the Red Cross (ICRC) commentaries support this nuance, suggesting that actions strictly limited to the interception of projectiles do not necessarily constitute an ``attack'' in the fullest sense if they do not target the adversary's personnel or territory. This legal grey zone is where the Sentinel operates.

Finally, Article 36 of Additional Protocol I imposes a binding obligation on states to determine whether the employment of a new weapon would, in some or all circumstances, be prohibited by international law. This creates the requirement for "Traceability" (or Explainability) in AI systems—a requirement that creates a bridge between legal compliance and software engineering.

\subsection{Contemporary AI Ethics: The Control Problem and Responsibility}
In the domain of AI safety, Stuart Russell identifies the ``Control Problem'' as the central existential risk: the difficulty of specifying an objective function that acts as we intend, rather than just as we command \cite{russell2019}. A system instructed to ``Defend the Base'' might theoretically decide that the most efficient way to do so is to preemptively strike all approaching entities, regardless of intent. This is a classic case of what Bostrom calls ``perverse instantiation'' \cite{bostrom2014}.

Paul Scharre reframes the debate around autonomy, arguing that the ``Human-in-the-loop'' model is becoming obsolete for hypersonic defense due to human physiological limits \cite{scharre2018}. He proposes ``Human-on-the-loop'' (supervisory control) as the necessary evolution. This raises the critical issue of the ``Responsibility Gap,'' first articulated by measures such as Robert Sparrow \cite{sparrow2007}, who argues that if a machine targets appropriately, it is not a moral agent, and if it targets inappropriately, no human can be held responsible if they did not order the error. Madeleine Elish refines this with the concept of the ``Moral Crumple Zone,'' where human operators are often unfairly held responsible for the failures of autonomous systems they could not meaningfully control \cite{elish2019}. A robust ethical framework for defensive AI must therefore shift liability from the reactive operator to the proactive designer and the rules of engagement themselves.

\subsection{Synthesis: The Defensive Gap}
Synthesizing these fields reveals a glaring gap. We have laws for humans (IHL), laws for slaves (Asimov), and fears of gods (Superintelligence). But we lack a specific, operational ethic for the \textit{Shield}—an autonomous system that is lethal yet purely defensive, rapid yet accountable, and distinctly separate from the logic of the \textit{Sword}. The "Sentinel" ruleset aims to fill this void.
