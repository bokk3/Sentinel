% Literature Review

The ethical governance of autonomous systems in warfare is a field characterized by a collision of disciplines: science fiction philosophy, classical military ethics, international law, and modern computer science. This review synthesizes these distinct threads to demonstrate that while significant work has been done on ``killer robots'' (offensive LAWS), there remains a critical theoretical vacuum regarding purely defensive, high-speed autonomous systems.

\subsection{The Asimovian Legacy and its Military Inadequacies}
Isaac Asimov's Three Laws of Robotics have served as the default starting point for machine ethics for nearly a century. However, as Susan Leigh Anderson argues, Asimov's laws were never intended as a functional ethical code, but rather as a literary device designed to generate conflict \cite{anderson2008}. Anderson notes that the laws effectively reduce machines to the status of ``ethical slaves,'' a stance that may be philosophically tenable for a household servant but becomes problematic in the chaos of the battlefield.

The primary failure of the Asimovian framework in a military context is the First Law's absolute prohibition on harm (``A robot may not injure a human being''). In a defensive engagement, minimizing overall harm often requires the sanctioned use of force—for example, destroying an incoming missile even if the debris risks injuring a bystander, provided that the alternative (the missile impact) would kill significantly more people. A strict First Law robot would be paralyzed by this ``Trolley Problem,'' unable to act. Furthermore, Asimov’s laws assume a clear distinction between ``human'' and ``non-human,'' a distinction that becomes blurred in modern warfare where combatants are often remote or obscured.

\subsection{Just War Theory: From \textit{Bellum} to \textit{Vim}}
Classical Just War Theory, rooted in the works of Augustine and Aquinas and modernized by Michael Walzer, provides the moral bedrock for Western military practice. Walzer’s distinction between \textit{Jus ad Bellum} (justice of going to war) and \textit{Jus in Bello} (justice in conduct) is crucial \cite{walzer1977}. Defensive AI systems sit firmly within the most accepted precept of \textit{Jus ad Bellum}: the right of self-defense. Unlike offensive systems, which must justify aggression, a defensive system is moral by its very existence, provided it remains defensive.

However, the nature of modern defensive force challenges classical definitions. Daniel Brunstetter introduces the concept of \textit{Jus ad Vim} (force short of war), arguing that modern technologies like drones and precision strikes operate in a ``grey zone'' where the scale of violence does not rise to full-scale war \cite{brunstetter2013}. High-speed intercepts fall into this category—they are acts of violence, but their intent is negation rather than destruction. The ethical framework for AI must therefore navigate this grey zone, ensuring that acts of ``force short of war'' do not inadvertently escalate into full-scale conflict.

\subsection{International Humanitarian Law (IHL) and the Definition of Attack}
The legal constraints on autonomous systems are defined primarily by the Additional Protocols to the Geneva Conventions. A critical but often overlooked distinction is found in Article 49 of Additional Protocol I, which defines an ``attack'' as an act of violence against the adversary, whether in offense or defense \cite{icrc}.

The International Committee of the Red Cross (ICRC) commentaries clarify that actions strictly limited to the interception of projectiles do not necessarily constitute an ``attack'' in the legal sense, but rather "neutralization." This legal nuance is fundamental to the "Sentinel" concept. If an AI system acts solely to neutralize a threat without targeting the human operator or platform, it may operate under a more permissive legal regime than a standard weapon system. However, Article 48 (Distinction) remains absolute: the system must distinguish between military objectives (the missile) and protected persons (civilians). The challenge for AI is translating this legal principle into a statistical threshold (e.g., confidence intervals).

Finally, Article 36 imposes a binding obligation on states to determine whether the employment of a new weapon would, in some or all circumstances, be prohibited by international law. This creates the requirement for "Traceability" (or Explainability) in AI systems—a requirement that creates a bridge between legal compliance and software engineering.

\subsection{Contemporary AI Ethics: The Control Problem and Responsibility}
In the domain of AI safety, Stuart Russell identifies the ``Control Problem'' as the central existential risk: the difficulty of specifying an objective function that acts as we intend, rather than just as we command \cite{russell2019}. A system instructed to ``Defend the Base'' might theoretically decide that the most efficient way to do so is to preemptively strike all approaching entities, regardless of intent. This is a classic case of what Bostrom calls ``perverse instantiation'' \cite{bostrom2014}.

Paul Scharre reframes the debate around autonomy, arguing that the ``Human-in-the-loop'' model is becoming obsolete for hypersonic defense due to human physiological limits \cite{scharre2018}. He proposes ``Human-on-the-loop'' (supervisory control) as the necessary evolution. However, Madeleine Elish warns of the ``Moral Crumple Zone,'' where human operators are legally held responsible for the failures of autonomous systems they could not meaningfully control \cite{elish2019}. A robust ethical framework for defensive AI must therefore shift liability from the reactive operator to the proactive designer and the rules of engagement themselves.

\subsection{Synthesis: The Defensive Gap}
Synthesizing these fields reveals a glaring gap. We have laws for humans (IHL), laws for slaves (Asimov), and fears of gods (Superintelligence). But we lack a specific, operational ethic for the \textit{Shield}—an autonomous system that is lethal yet purely defensive, rapid yet accountable, and distinctly separate from the logic of the \textit{Sword}. The "Sentinel" ruleset aims to fill this void.
