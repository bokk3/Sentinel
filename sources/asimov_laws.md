# Research Notes: Asimov's Three Laws & Military AI

**Date:** 2025-12-16
**Topic:** Asimov's Three Laws of Robotics and critical analysis in military context.
**Phase:** 1.1 Literature Review

## 1. The Laws

### The Three Laws of Robotics (Asimov, 1942)

1.  **First Law:** A robot may not injure a human being or, through inaction, allow a human being to come to harm.
2.  **Second Law:** A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.
3.  **Third Law:** A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.

### The Zeroth Law (Asimov, 1985)

0.  **Zeroth Law:** A robot may not harm humanity, or, through inaction, allow humanity to come to harm.

## 2. Critical Analysis for Military AI

### Fundamental Incompatibility

- **The First Law Problem:** Defensive systems are often lethal. A system designed to intercept a missile might inadvertently harm a human pilot or be part of a kill chain. Fully autonomous defensive weeds (like CIWS) technically violate the First Law if they engage a manned target.
- **Intention:** Military funding often explicitly seeks systems capable of projecting force, which is the antithesis of the First Law.

### Interpretative Challenges

- **"Human" vs. "Combatant":** In a war zone, how does an AI distinguish a "human to be protected" from a "valid target"? Asimov's laws make no distinction between friend and foe.
- **"Harm":** Does allowing an enemy tank to pass "allow harm via inaction" to friendly troops? This creates a paralysis loop.
- **"Humanity":** The Zeroth Law is even more dangerous. An AI might decide that the "best" way to protect humanity is to preemptively strike a perceived threat, or even subjugate humanity to prevent self-destruction (the theme of the movie _I, Robot_).

### Operational Dilemmas

- **Trolley Problems:** Defensive AI often has to weigh one risk against another (e.g., shoot down a missile over a populated area vs. let it hit a high-value target). Asimov's laws are absolute and struggle with utilitarian calculus.
- **Chain of Command (Second Law):** Who is the "human" to obey? The local commander? The head of state? A hacker spoofing authentication?

### Literature & Policy Connections

- **Darpa ASIMOV:** (Autonomy Standards and Ideals with Military Operational Values) - Attempting to translate ethical impulses into code, dealing with the "unintended consequences" Asimov warned about.
- **Narrative vs. Engineering:** Robotics experts (Brooks, Arkin) often critique the laws as literary devices that assume human-like reasoning capabilities that do not exist.

## Sources Reviewed

- [Encyclopedia Britannica - Three Laws of Robotics](https://www.britannica.com/topic/Three-Laws-of-Robotics)
- [Brookings - Isaac Asimov's Laws of Robotics are wrong](https://www.brookings.edu/articles/isaac-asimovs-laws-of-robotics-are-wrong/)
- [DARPA ASIMOV Program](https://www.darpa.mil/program/autonomy-standards-and-ideals-with-military-operational-values)
- General search results on "Asimov Three Laws military critical analysis"
