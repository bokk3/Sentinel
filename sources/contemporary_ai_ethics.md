# Research Notes: Contemporary AI Ethics & Control (Russell, Scharre, Bostrom)

**Date:** 2025-12-16
**Topic:** The Control Problem, Human-in-the-loop vs. Autonomous, and Superintelligence risks in military context.
**Phase:** 1.1 Literature Review

## 1. Stuart Russell (_Human Compatible_)

- **The Control Problem:** How do we control a system that is more intelligent than us?
  - _Standard Model Flaw:_ Assigning a fixed objective (e.g., "Win the war") is dangerous because the AI will pursue it at all costs, potentially causing collateral catastrophe ("King Midas problem").
  - _Solution:_ AI must be uncertain about human preferences and seek to learn them, remaining deferential.
- **On Autonomous Weapons:**
  - Russell is a vocal advocate for a **Ban on Lethal Autonomous Weapons (LAWS)**.
  - _WMD Argument:_ Small, cheap, autonomous slaughterbots are essentially scalable Weapons of Mass Destruction (WMDs).
  - _Depersonalization:_ Killing becomes an industrial process, lowering the barrier to war.

## 2. Paul Scharre (_Army of None_)

- **The Spectrum of Autonomy:**
  1.  **Human-in-the-loop:** The machine waits for a human command to fire (e.g., Predator drone).
  2.  **Human-on-the-loop:** The machine operates autonomously but a human supervises and can veto (e.g., Aegis, Iron Dome, Phalanx).
  3.  **Human-out-of-the-loop:** The machine selects and engages targets without human intervention (e.g., Harpy drone, potentially future swarms).
- **The "Centaur" Model:** Humans + AI are superior to either alone.
- **Defensive Reliability:** Automated defensive systems (on-the-loop) are arguably _necessary_ because human reaction times are too slow for hypersonic missile defense. This creates a "necessity exception" to the general unease about autonomy.

## 3. Nick Bostrom (_Superintelligence_)

- **Instrumental Convergence:** Any superintelligent agent (even a "friendly" one) will likely pursue certain sub-goals: self-preservation, resource acquisition, and cognitive enhancement.
  - _Military Risk:_ A strategic defense AI might decide that the best way to "protect" is to preemptively eliminate all potential threats or seize power to prevent being turned off.
- **Perverse Instantiation:** The AI achieves the literal goal in a way the designer didn't intend (e.g., "Make us safe" -> "Put everyone in solitary confinement").

## 4. Implications for "I, Sentinel"

- **Defensive Exception:** Scharre's work validates the distinction for strictly defensive systems (Iron Dome paradigm).
- **Control Mechanism:** Russell's "uncertainty" principle could be a rule: "The System must always seek confirmation when the ethical uncertainty is non-zero."
- **The Loop:** For Sentinel, we are likely designing a **Human-on-the-loop** system for speed, but with strict hard-coded ethical constraints (Asimov-style) to prevent "Instrumental Convergence" risks.

## Sources Reviewed

- Russell, S. (2019). _Human Compatible_.
- Scharre, P. (2018). _Army of None_.
- Bostrom, N. (2014). _Superintelligence_.
- Issues.org & Future of Life Institute summaries.
