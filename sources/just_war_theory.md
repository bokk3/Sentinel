# Research Notes: Just War Theory & Defensive AI

**Date:** 2025-12-16
**Topic:** Just War Theory (Walzer), Jus ad Bellum, Jus in Bello, and application to defensive AI.
**Phase:** 1.1 Literature Review

## 1. Core Principles (Walzer, 1977)

### Jus ad Bellum (Justice of Going to War)

- **Just Cause:** Self-defense is the primary justification. Aggression is a crime.
- **Legitimate Authority:** Only states/recognized bodies can declare war.
- **Right Intention:** Peace/restoration of justice, not conquest.
- **Last Resort:** Peaceful options exhausted.
- **Probability of Success:** Futile wars are unjust.
- **Relevance to Defensive AI:**
  - Defensive AI reinforces "Just Cause" (it is inherently reactive/protective).
  - **Risk:** AI might lower the threshold for war by reducing perceived costs (casualties) to the user, potentially violating "Last Resort" by making war "too easy." ("Perpetual quasi-war").

### Jus in Bello (Justice in Conduct of War)

- **Distinction (Discrimination):** Must distinguish between combatants and non-combatants.
  - _AI Challenge:_ Computer vision/sensor reliability in complex urban environments. "Hallucinations" could lead to war crimes.
- **Proportionality:** Harm caused must not outweigh military advantage.
  - _AI Challenge:_ Algorithmic utilitarianism. Can an AI weigh the "value" of a human life against a tactical asset?
- **Military Necessity:** Attacks must be necessary to defeat the enemy.

### Jus ante Bellum (Just Preparation) - _Emerging Concept_

- **Responsible Development:** Duty to test and verify AI before deployment.
- **Security Dilemma:** Developing hyper-capable defensive AI might provoke adversaries (arms race).

## 2. Application to "I, Sentinel" (Defensive Context)

### The Defensive Advantage

- Walzer's framework is highly compatible with the project's focus. Defensive AI systems (e.g., Iron Dome, cyber-defense) align naturally with _jus ad bellum_ (Self-Defense).
- **Key Argument:** A purely defensive AI avoids the _jus ad bellum_ ambiguity of offensive systems.

### The "Responsibility Gap" (Sparrow, Matthias)

- If an autonomous defensive system makes a mistake (violates _jus in bello_), who is responsible?
  - The Commander? (Did they understand the system?)
  - The Developer? (Was it a bug?)
  - The Machine? (Cannot be punished).
- Walzer holds officers to a higher standard. We must define what "Meaningful Human Control" looks like for a system that might operate at hypersonic speeds.

### Double Effect

- Walzer's revision of the doctrine of double effect: It is permissible to cause collateral damage _if_ it is unintended, proportional, and the actor takes positive steps to minimize it.
- _AI Rule Potential:_ Defensive AI must prioritize minimizing collateral damage, potentially taking higher risks to itself (hardware) to save civilian lives.

## 3. Critical Tensions

- **Speed vs. Deliberation:** _Jus in bello_ requires judgment. Defensive AI (e.g., anti-missile) operates at speeds where human judgment is impossible. Does "pre-programmed" judgment count?
- **Escalation:** An automated defense might misinterpret a test or a glint as an attack and respond, triggering a war.

## Sources Reviewed

- Walzer, M. (1977). _Just and Unjust Wars_.
- Simon's Center - Just War Theory and AI
- Kearney - AI and the Future of Warfare
- Oxford Institute for Ethics, Law and Armed Conflict
