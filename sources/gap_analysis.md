# Research Notes: Gap Analysis

**Date:** 2025-12-16
**Topic:** Identifying gaps in existing frameworks (Asimov, Just War, IHL, Policy) to position "I, Sentinel".
**Phase:** 1.1 Literature Review

## 1. The "Defensive Specificity" Gap

- **Current State:** Most literature (Russell, UN GGE, HRW) treats "Lethal Autonomous Weapons Systems" (LAWS) as a monolith, often focusing on offensive "slaughterbots" or loitering munitions.
- **The Gap:** There is a lack of rigorous ethical frameworks specifically tailored for **purely defensive** high-speed systems (e.g., AI-driven missile defense, cyber-defense).
- **Our Contribution:** We can argue that _defensive_ autonomy requires a distinct ethical separation from _offensive_ autonomy, as it aligns naturally with _jus ad bellum_ (self-defense) and fundamentally changes the risk/benefit calculus.

## 2. The "Operational Asimov" Gap

- **Current State:** Asimov's laws are widely dismissed by serious roboticists (Brooks, Arkin) as literary plot devices that are technically impossible to implement (too vague, require general intelligence).
- **The Gap:** No one has successfully adapted the _structure_ of hierarchical axioms into a **workable, verifiable military protocol**.
- **Our Contribution:** We will propose a "Sentinel Ruleset" that takes the _spirit_ of Asimov (hierarchy of constraints) but replaces the vague natural language with specific, verifiable military constraints (e.g., "Positive ID Requirement" > "Mission Accomplishment").

## 3. The "Appropriate Judgment" Definition Gap

- **Current State:** DoD 3000.09 and NATO strategy require "appropriate levels of human judgment" but leave "appropriate" undefined.
- **The Gap:** Does "appropriate" mean a human must press the button for every intercept? (Impossible for hypersonic defense). Or does it mean the human authorizes the _parameters_ of the system?
- **Our Contribution:** We can offer a concrete definition of "Meaningful Human Control" for defensive AI: **Control via Pre-Delegated Constraints**, not real-time intervention.

## 4. The "Machine Martyrdom" Gap

- **Current State:** IHL focuses on minimizing civilian harm (Proportionality) and protecting combatants (hors de combat).
- **The Gap:** Few frameworks explicitly discuss the moral obligation of a defensive AI to **sacrifice itself** (or expensive material assets) to reduce the probability of civilian harm.
- **Our Contribution:** A rule mandating "Material Sacrifice" â€“ the system must prioritize saving human life (even enemy life, if possible) over its own survival or the survival of the asset it protects, contradicting standard military doctrine of "force protection" in favor of "human protection."

## Summary of Sentinel's Niche

> "I, Sentinel" will bridge the gap between high-level ethical philosophy (Walzer/Asimov) and technical engineering reality (DoD/NATO) by proposing a **concrete, hierarchical ruleset specifically for defensive AI** that operationalizes "appropriate judgment" through pre-delegated constraint.
